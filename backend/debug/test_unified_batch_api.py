#!/usr/bin/env python3
"""
Comprehensive Test Suite for Unified Batch API
Senior Principal Engineer Implementation

Tests the enterprise-grade unified batch API that consolidates
7 fragmented files and 30+ endpoints into a single intelligent interface.
"""

import asyncio
import json
from unittest.mock import AsyncMock, MagicMock, patch
from typing import Dict, Any

async def test_unified_batch_api_models():
    """Test the unified API request/response models"""
    print("üìã Testing Unified Batch API Models")
    print("=" * 60)
    
    try:
        from pydantic import ValidationError
        
        # Test imports without full dependency chain
        from api.unified_batch_api import (
            LigandInput, BatchConfigurationModel, UnifiedBatchSubmissionRequest,
            BatchSubmissionResponse, BatchStatusResponse
        )
        
        # Test 1: Ligand input validation
        print("\nüß™ Testing ligand input validation...")
        
        # Valid ligand
        valid_ligand = LigandInput(name="Aspirin", smiles="CC(=O)OC1=CC=CC=C1C(=O)O")
        assert valid_ligand.name == "Aspirin", "Ligand name should be set"
        assert valid_ligand.smiles == "CC(=O)OC1=CC=CC=C1C(=O)O", "SMILES should be set"
        
        # Invalid ligand (empty SMILES)
        try:
            invalid_ligand = LigandInput(name="Invalid", smiles="   ")
            assert False, "Should have raised validation error for empty SMILES"
        except ValidationError as e:
            assert "SMILES string cannot be empty" in str(e), "Should catch empty SMILES"
        
        print("   ‚úÖ Ligand input validation working")
        print(f"   Valid ligand: {valid_ligand.name} - {valid_ligand.smiles[:20]}...")
        
        # Test 2: Batch configuration validation  
        print("\n‚öôÔ∏è Testing batch configuration...")
        
        from api.unified_batch_api import BatchPriority, BatchSchedulingStrategy
        
        config = BatchConfigurationModel(
            priority=BatchPriority.HIGH,
            scheduling_strategy=BatchSchedulingStrategy.RESOURCE_AWARE,
            max_concurrent_jobs=8,
            retry_failed_jobs=True,
            enable_predictive_completion=True
        )
        
        assert config.priority == BatchPriority.HIGH, "Priority should be HIGH"
        assert config.max_concurrent_jobs == 8, "Max concurrent should be 8"
        assert config.retry_failed_jobs == True, "Retry should be enabled"
        
        print("   ‚úÖ Batch configuration validation working")
        print(f"   Priority: {config.priority.value}")
        print(f"   Strategy: {config.scheduling_strategy.value}")
        
        # Test 3: Unified batch submission request
        print("\nüìù Testing batch submission request...")
        
        ligands = [
            LigandInput(name="Compound A", smiles="CCO"),
            LigandInput(name="Compound B", smiles="CCC"),
            LigandInput(name="Compound C", smiles="CCCC")
        ]
        
        request = UnifiedBatchSubmissionRequest(
            job_name="API Test Batch",
            protein_sequence="MKLLVVLALVLGFLLFSASAAQQEGEVAVRFCSVDNCASDNKQGPCALAHPKEAKPPRRRPGGGSLALSLGGGAATAGSGSGSGSAALAALPAAFGGAAKKLLVVALAVLSASAAA",
            protein_name="Test Protein",
            ligands=ligands,
            model_name="boltz2",
            configuration=config,
            metadata={"test": "unified_api"}
        )
        
        assert request.job_name == "API Test Batch", "Job name should match"
        assert len(request.ligands) == 3, "Should have 3 ligands"
        assert request.configuration == config, "Configuration should match"
        assert request.metadata["test"] == "unified_api", "Metadata should be preserved"
        
        print("   ‚úÖ Batch submission request validation working")
        print(f"   Job: {request.job_name}")
        print(f"   Ligands: {len(request.ligands)}")
        print(f"   Protein length: {len(request.protein_sequence)} AA")
        
        # Test 4: Invalid request validation
        print("\n‚ùå Testing invalid request handling...")
        
        # Too many ligands
        try:
            too_many_ligands = [LigandInput(smiles=f"C{'C'*i}O") for i in range(101)]  # 101 ligands
            invalid_request = UnifiedBatchSubmissionRequest(
                job_name="Too Large",
                protein_sequence="MKLLVVLALVLGFLLFSASAAQQEGEVAVRFC",
                protein_name="Test",
                ligands=too_many_ligands
            )
            assert False, "Should have raised validation error for too many ligands"
        except ValidationError as e:
            assert "Too many ligands" in str(e), "Should catch ligand limit"
        
        # Empty job name
        try:
            empty_name_request = UnifiedBatchSubmissionRequest(
                job_name="",
                protein_sequence="MKLLVVLALVLGFLLFSASAAQQEGEVAVRFC",
                protein_name="Test",
                ligands=[LigandInput(smiles="CCO")]
            )
            assert False, "Should have raised validation error for empty job name"
        except ValidationError as e:
            assert "ensure this value has at least 1 character" in str(e), "Should catch empty job name"
        
        print("   ‚úÖ Invalid request validation working")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå API models test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_batch_submission_logic():
    """Test the batch submission endpoint logic"""
    print("\nüöÄ Testing Batch Submission Logic")
    print("=" * 60)
    
    try:
        # Import API components
        from api.unified_batch_api import (
            submit_unified_batch, UnifiedBatchSubmissionRequest,
            LigandInput, BatchConfigurationModel, BatchPriority
        )
        
        # Create test request
        test_ligands = [
            LigandInput(name="Test Ligand 1", smiles="CCO"),
            LigandInput(name="Test Ligand 2", smiles="CCC")
        ]
        
        config = BatchConfigurationModel(
            priority=BatchPriority.NORMAL,
            max_concurrent_jobs=3
        )
        
        request = UnifiedBatchSubmissionRequest(
            job_name="Logic Test Batch",
            protein_sequence="MKLLVVLALVLGFLLFSASAAQQEGEVAVRFCSVDNCASDNKQGPCALAHPKEAKPPRRRPGGGSLALSLGGGAATAGSGSGSGSAALAALPAAFGGAAKKLLVVALAVLSASAAA",
            protein_name="Logic Test Protein",
            ligands=test_ligands,
            configuration=config
        )
        
        # Mock the unified batch processor
        print("\nüîß Testing with mocked unified batch processor...")
        
        mock_result = {
            'success': True,
            'batch_id': 'test_batch_12345',
            'batch_parent': {'name': 'Logic Test Batch'},
            'total_jobs': 2,
            'execution_plan': {
                'estimated_duration': 600.0,
                'scheduling_strategy': 'adaptive',
                'optimization_recommendations': ['Enable parallel processing']
            },
            'execution_details': {
                'started_jobs': 2,
                'queued_jobs': 0
            },
            'risk_assessment': {'risk_score': 0.2}
        }
        
        with patch('api.unified_batch_api.unified_batch_processor') as mock_processor:
            mock_processor.submit_batch = AsyncMock(return_value=mock_result)
            
            # Create mock background tasks
            from fastapi import BackgroundTasks
            background_tasks = BackgroundTasks()
            
            # Call the submission endpoint
            response = await submit_unified_batch(request, background_tasks)
            
            # Verify response structure
            assert response.success == True, "Response should indicate success"
            assert response.batch_id == 'test_batch_12345', "Batch ID should match"
            assert response.total_jobs == 2, "Should have 2 total jobs"
            assert response.estimated_duration_seconds == 600.0, "Duration should match"
            assert response.scheduling_strategy == 'adaptive', "Strategy should match"
            assert response.started_jobs == 2, "Should have started 2 jobs"
            assert response.queued_jobs == 0, "Should have 0 queued jobs"
            
            # Verify URL construction
            assert response.status_url == "/api/v3/batches/test_batch_12345/status", "Status URL correct"
            assert response.results_url == "/api/v3/batches/test_batch_12345/results", "Results URL correct"
            
            # Verify processor was called correctly
            assert mock_processor.submit_batch.called, "Processor should be called"
            call_args = mock_processor.submit_batch.call_args[0][0]
            assert call_args.job_name == request.job_name, "Job name should be passed"
            assert len(call_args.ligands) == len(request.ligands), "Ligands should be passed"
            
            print("   ‚úÖ Batch submission logic working")
            print(f"   Batch ID: {response.batch_id}")
            print(f"   Total jobs: {response.total_jobs}")
            print(f"   Started jobs: {response.started_jobs}")
            print(f"   Recommendations: {len(response.optimization_recommendations)}")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Batch submission logic test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_batch_status_logic():
    """Test the batch status endpoint logic"""
    print("\nüìä Testing Batch Status Logic")
    print("=" * 60)
    
    try:
        from api.unified_batch_api import get_batch_status
        from datetime import datetime
        
        # Mock batch processor status response
        mock_status_result = {
            'batch_parent': {
                'id': 'test_batch_status',
                'name': 'Status Test Batch',
                'status': 'running',
                'created_at': 1703980800,  # timestamp
                'updated_at': 1703984400
            },
            'child_jobs': [
                {
                    'id': 'child_1',
                    'name': 'Status Test Batch - Compound A',
                    'status': 'completed',
                    'created_at': 1703980800,
                    'started_at': 1703980860,
                    'completed_at': 1703981160,
                    'input_data': {
                        'ligand_name': 'Compound A',
                        'ligand_smiles': 'CCO'
                    },
                    'has_results': True
                },
                {
                    'id': 'child_2', 
                    'name': 'Status Test Batch - Compound B',
                    'status': 'running',
                    'created_at': 1703980800,
                    'started_at': 1703980920,
                    'input_data': {
                        'ligand_name': 'Compound B',
                        'ligand_smiles': 'CCC'
                    },
                    'has_results': False
                }
            ],
            'progress': {
                'total': 2,
                'completed': 1,
                'running': 1,
                'failed': 0,
                'pending': 0,
                'progress_percentage': 50.0
            },
            'insights': {
                'batch_health': 'healthy',
                'performance_rating': 'good',
                'estimated_completion_time': 300.0
            },
            'execution_plan': {
                'scheduling_strategy': 'adaptive',
                'estimated_duration': 600.0
            }
        }
        
        print("\nüìà Testing status retrieval with mock data...")
        
        with patch('api.unified_batch_api.unified_batch_processor') as mock_processor:
            mock_processor.get_batch_status = AsyncMock(return_value=mock_status_result)
            
            # Call the status endpoint
            response = await get_batch_status('test_batch_status')
            
            # Verify response structure
            assert response.batch_id == 'test_batch_status', "Batch ID should match"
            assert response.batch_name == 'Status Test Batch', "Batch name should match"
            assert response.status == 'running', "Status should match"
            assert response.total_jobs == 2, "Total jobs should match"
            
            # Verify progress tracking
            assert response.progress['total'] == 2, "Progress total should match"
            assert response.progress['completed'] == 1, "Progress completed should match"
            assert response.progress['progress_percentage'] == 50.0, "Progress percentage should match"
            
            # Verify job summaries
            assert len(response.jobs) == 2, "Should have 2 job summaries"
            
            job1 = response.jobs[0]
            assert job1.job_id == 'child_1', "First job ID should match"
            assert job1.status == 'completed', "First job should be completed"
            assert job1.ligand_name == 'Compound A', "Ligand name should match"
            assert job1.results_available == True, "Results should be available"
            
            job2 = response.jobs[1]
            assert job2.job_id == 'child_2', "Second job ID should match"
            assert job2.status == 'running', "Second job should be running"
            assert job2.results_available == False, "Results should not be available"
            
            # Verify insights
            assert response.insights['batch_health'] == 'healthy', "Health should match"
            assert response.insights['performance_rating'] == 'good', "Performance should match"
            assert response.estimated_completion == 300.0, "Completion estimate should match"
            
            print("   ‚úÖ Batch status logic working")
            print(f"   Batch: {response.batch_name}")
            print(f"   Status: {response.status}")
            print(f"   Progress: {response.progress['completed']}/{response.progress['total']}")
            print(f"   Health: {response.insights['batch_health']}")
            print(f"   Jobs with results: {sum(1 for job in response.jobs if job.results_available)}")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Batch status logic test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_api_error_handling():
    """Test API error handling and validation"""
    print("\nüõ°Ô∏è Testing API Error Handling")
    print("=" * 60)
    
    try:
        from fastapi import HTTPException
        from api.unified_batch_api import get_batch_status, get_batch_results
        
        # Test 1: Batch not found error
        print("\n‚ùå Testing batch not found error...")
        
        with patch('api.unified_batch_api.unified_batch_processor') as mock_processor:
            mock_processor.get_batch_status = AsyncMock(return_value={'error': 'Batch not found'})
            
            try:
                await get_batch_status('nonexistent_batch')
                assert False, "Should have raised HTTPException for not found"
            except HTTPException as e:
                assert e.status_code == 404, "Should return 404 status code"
                assert "Batch not found" in str(e.detail), "Should include error message"
        
        print("   ‚úÖ Batch not found error handling working")
        
        # Test 2: Internal server error
        print("\nüí• Testing internal server error...")
        
        with patch('api.unified_batch_api.unified_batch_processor') as mock_processor:
            mock_processor.get_batch_status = AsyncMock(side_effect=Exception("Database connection failed"))
            
            try:
                await get_batch_status('error_batch')
                assert False, "Should have raised HTTPException for server error"
            except HTTPException as e:
                assert e.status_code == 500, "Should return 500 status code"
                assert "Failed to retrieve batch status" in str(e.detail), "Should include error message"
        
        print("   ‚úÖ Internal server error handling working")
        
        # Test 3: Validation error handling would be tested at FastAPI level
        print("\n‚úÖ Testing validation error simulation...")
        
        from pydantic import ValidationError
        
        # This simulates what would happen with invalid input
        try:
            from api.unified_batch_api import LigandInput
            invalid_ligand = LigandInput(smiles="")  # Empty SMILES
            assert False, "Should raise validation error"
        except ValidationError as e:
            assert "ensure this value has at least 3 characters" in str(e), "Should catch SMILES length"
        
        print("   ‚úÖ Validation error handling working")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Error handling test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_api_response_formats():
    """Test API response formats and structure"""
    print("\nüìã Testing API Response Formats")
    print("=" * 60)
    
    try:
        from api.unified_batch_api import (
            BatchSubmissionResponse, BatchStatusResponse, BatchResultsResponse
        )
        from datetime import datetime
        
        # Test 1: Batch submission response format
        print("\nüìù Testing submission response format...")
        
        submission_response = BatchSubmissionResponse(
            success=True,
            batch_id="format_test_12345",
            message="Batch submitted successfully",
            total_jobs=5,
            estimated_duration_seconds=1500.0,
            scheduling_strategy="adaptive",
            started_jobs=3,
            queued_jobs=2,
            optimization_recommendations=["Enable parallel processing", "Use resource-aware scheduling"],
            risk_assessment={"risk_score": 0.15, "factors": ["Large batch size"]},
            status_url="/api/v3/batches/format_test_12345/status",
            results_url="/api/v3/batches/format_test_12345/results"
        )
        
        # Verify all fields are present and correctly typed
        assert submission_response.success == True, "Success field should be boolean"
        assert isinstance(submission_response.batch_id, str), "Batch ID should be string"
        assert isinstance(submission_response.total_jobs, int), "Total jobs should be integer"
        assert isinstance(submission_response.estimated_duration_seconds, float), "Duration should be float"
        assert len(submission_response.optimization_recommendations) == 2, "Should have 2 recommendations"
        
        print("   ‚úÖ Submission response format correct")
        print(f"   Batch ID: {submission_response.batch_id}")
        print(f"   Jobs: {submission_response.started_jobs} started, {submission_response.queued_jobs} queued")
        
        # Test 2: Status response format
        print("\nüìä Testing status response format...")
        
        from api.unified_batch_api import BatchJobSummary
        
        job_summaries = [
            BatchJobSummary(
                job_id="job_1",
                name="Test Job 1",
                status="completed",
                ligand_name="Compound A",
                ligand_smiles="CCO",
                created_at=datetime.now(),
                completed_at=datetime.now(),
                error_message=None,
                results_available=True
            ),
            BatchJobSummary(
                job_id="job_2",
                name="Test Job 2", 
                status="running",
                ligand_name="Compound B",
                ligand_smiles="CCC",
                created_at=datetime.now(),
                started_at=datetime.now(),
                completed_at=None,
                error_message=None,
                results_available=False
            )
        ]
        
        status_response = BatchStatusResponse(
            batch_id="format_test_12345",
            batch_name="Format Test Batch",
            status="running",
            created_at=datetime.now(),
            progress={
                "total": 2,
                "completed": 1,
                "running": 1,
                "progress_percentage": 50.0
            },
            total_jobs=2,
            jobs=job_summaries,
            insights={
                "batch_health": "healthy",
                "performance_rating": "good"
            },
            estimated_completion=300.0
        )
        
        # Verify status response structure
        assert isinstance(status_response.progress, dict), "Progress should be dictionary"
        assert status_response.progress["total"] == 2, "Progress total should match"
        assert len(status_response.jobs) == 2, "Should have 2 job summaries"
        assert status_response.jobs[0].results_available == True, "First job should have results"
        assert status_response.jobs[1].results_available == False, "Second job should not have results"
        
        print("   ‚úÖ Status response format correct")
        print(f"   Progress: {status_response.progress['completed']}/{status_response.progress['total']}")
        print(f"   Health: {status_response.insights['batch_health']}")
        
        # Test 3: JSON serialization
        print("\nüîÑ Testing JSON serialization...")
        
        # Convert responses to JSON to ensure they serialize correctly
        submission_json = submission_response.json()
        status_json = status_response.json()
        
        # Parse back to ensure valid JSON
        submission_data = json.loads(submission_json)
        status_data = json.loads(status_json)
        
        assert submission_data["success"] == True, "JSON should preserve boolean values"
        assert status_data["batch_id"] == "format_test_12345", "JSON should preserve string values"
        assert len(status_data["jobs"]) == 2, "JSON should preserve arrays"
        
        print("   ‚úÖ JSON serialization working")
        print(f"   Submission JSON size: {len(submission_json)} chars")
        print(f"   Status JSON size: {len(status_json)} chars")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Response format test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """Run all Unified Batch API tests"""
    print("üöÄ UNIFIED BATCH API TEST SUITE")
    print("Senior Principal Engineer Implementation")
    print("=" * 70)
    print("Testing enterprise-grade unified batch API consolidation")
    print("=" * 70)
    
    tests = [
        ("API Models and Validation", test_unified_batch_api_models),
        ("Batch Submission Logic", test_batch_submission_logic),
        ("Batch Status Logic", test_batch_status_logic),
        ("Error Handling", test_api_error_handling),
        ("Response Formats", test_api_response_formats)
    ]
    
    passed = 0
    for test_name, test_func in tests:
        print(f"\nüß™ Running {test_name}...")
        try:
            if await test_func():
                passed += 1
                print(f"‚úÖ {test_name}: PASSED")
            else:
                print(f"‚ùå {test_name}: FAILED")
        except Exception as e:
            print(f"üí• {test_name}: CRASHED - {e}")
    
    print("\n" + "=" * 70)
    print(f"üìä RESULTS: {passed}/{len(tests)} test categories passed")
    
    if passed == len(tests):
        print("üéâ UNIFIED BATCH API READY FOR PRODUCTION!")
        print("üèÜ Successfully consolidated 7 fragmented API files")
        print("‚ö° 30+ endpoints reduced to intelligent unified interface")
        print("üöÄ Phase 3 complete - ready for Phase 4: Frontend Integration")
        return 0
    else:
        print("‚ö†Ô∏è Some API tests failed - review implementation")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)