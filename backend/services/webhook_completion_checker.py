"""
Webhook-First Completion Checker - Replaces polling with real-time notifications
Optimized for GKE + Modal architecture with immediate response to job completions
"""

import asyncio
import logging
import time
from typing import Dict, Any, Optional, Set, List
from datetime import datetime, timedelta
from dataclasses import dataclass

from services.batch_aware_completion_checker import BatchAwareCompletionChecker
from database.unified_job_manager import unified_job_manager
from services.production_modal_service import production_modal_service
from services.gcp_storage_service import gcp_storage_service

logger = logging.getLogger(__name__)

@dataclass
class WebhookStats:
    """Track webhook performance metrics"""
    total_webhooks_received: int = 0
    successful_completions: int = 0
    failed_completions: int = 0
    duplicate_webhooks: int = 0
    processing_errors: int = 0
    average_processing_time: float = 0.0
    last_webhook_time: Optional[datetime] = None

class WebhookCompletionChecker:
    """
    Webhook-first completion checker with polling fallback
    
    Primary architecture:
    - Webhooks handle 99% of completions in real-time
    - Minimal polling (every 5 minutes) as safety net
    - Intelligent duplicate detection
    - Batch-aware processing
    """
    
    def __init__(self):
        self.batch_checker = BatchAwareCompletionChecker()
        self.webhook_stats = WebhookStats()
        self.processed_call_ids: Set[str] = set()  # Prevent duplicates
        self.polling_enabled = True
        self.polling_interval = 300  # 5 minutes fallback polling
        self.last_poll_time = 0
        
        # Performance tracking
        self.processing_times: List[float] = []
        self.max_processing_times = 100  # Keep last 100 for averages
        
    async def on_webhook_completion(
        self,
        modal_call_id: str,
        status: str,
        result_data: Dict[str, Any],
        metadata: Dict[str, Any]
    ) -> bool:
        """
        Handle webhook completion - primary entry point
        
        Returns True if successfully processed, False otherwise
        """
        
        start_time = time.time()
        
        try:
            # Update stats
            self.webhook_stats.total_webhooks_received += 1
            self.webhook_stats.last_webhook_time = datetime.utcnow()
            
            # Check for duplicates
            if modal_call_id in self.processed_call_ids:
                logger.info(f"üîÑ Duplicate webhook ignored: {modal_call_id}")
                self.webhook_stats.duplicate_webhooks += 1
                return True
            
            # Find the job associated with this Modal call
            job_data = await self._find_job_by_modal_call_id(modal_call_id)
            if not job_data:
                logger.error(f"‚ùå Job not found for modal_call_id: {modal_call_id}")
                self.webhook_stats.processing_errors += 1
                return False
            
            job_id = job_data['id']
            logger.info(f"üéØ Webhook processing: {modal_call_id} -> job {job_id}")
            
            # Process the completion
            if status == 'success':
                await self._process_successful_completion(job_id, result_data, modal_call_id)
                self.webhook_stats.successful_completions += 1
            else:
                await self._process_failed_completion(job_id, result_data, modal_call_id)
                self.webhook_stats.failed_completions += 1
            
            # Mark as processed to prevent duplicates
            self.processed_call_ids.add(modal_call_id)
            
            # Update processing time metrics
            processing_time = time.time() - start_time
            self._update_processing_metrics(processing_time)
            
            logger.info(f"‚úÖ Webhook processed in {processing_time:.3f}s: {modal_call_id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error processing webhook {modal_call_id}: {e}")
            self.webhook_stats.processing_errors += 1
            return False
    
    async def _process_successful_completion(
        self,
        job_id: str,
        result_data: Dict[str, Any],
        modal_call_id: str
    ):
        """Process successful job completion"""
        
        try:
            # Update job status in database
            update_data = {
                'status': 'completed',
                'completed_at': time.time(),
                'modal_call_id': modal_call_id,
                'output_data': result_data,
                'completion_method': 'webhook',
                'webhook_processed_at': time.time()
            }
            
            success = unified_job_manager.update_job_status(job_id, 'completed', update_data)
            if not success:
                raise Exception(f"Failed to update job status for {job_id}")
            
            # Store results to GCP
            await self._store_results_to_gcp(job_id, result_data)
            
            # Trigger batch-aware completion processing
            await self.batch_checker.on_job_completion(job_id, result_data)
            
            # Notify production modal service
            await production_modal_service.on_completion(modal_call_id, result_data)
            
            logger.info(f"‚úÖ Job {job_id} completed successfully via webhook")
            
        except Exception as e:
            logger.error(f"‚ùå Error processing successful completion for {job_id}: {e}")
            raise
    
    async def _process_failed_completion(
        self,
        job_id: str,
        error_data: Dict[str, Any],
        modal_call_id: str
    ):
        """Process failed job completion"""
        
        try:
            # Update job status in database
            update_data = {
                'status': 'failed',
                'failed_at': time.time(),
                'modal_call_id': modal_call_id,
                'error_message': error_data.get('message', 'Unknown error'),
                'error_details': error_data,
                'completion_method': 'webhook',
                'webhook_processed_at': time.time()
            }
            
            success = unified_job_manager.update_job_status(job_id, 'failed', update_data)
            if not success:
                raise Exception(f"Failed to update job status for {job_id}")
            
            # Trigger batch-aware failure processing
            await self.batch_checker.on_job_failure(job_id, error_data)
            
            # Notify production modal service
            await production_modal_service.on_failure(modal_call_id, error_data)
            
            logger.info(f"‚ùå Job {job_id} failed via webhook: {error_data.get('message', 'Unknown error')}")
            
        except Exception as e:
            logger.error(f"‚ùå Error processing failed completion for {job_id}: {e}")
            raise
    
    async def _find_job_by_modal_call_id(self, modal_call_id: str) -> Optional[Dict[str, Any]]:
        """Find job by Modal call ID using production modal service"""
        
        try:
            # First try the production modal service cache
            execution = await production_modal_service.get_execution_status(modal_call_id)
            if execution:
                job_data = unified_job_manager.get_job(execution.job_id)
                if job_data:
                    return job_data
            
            # Fallback: search through recent jobs (this should be rare)
            logger.warning(f"‚ö†Ô∏è Fallback job search for modal_call_id: {modal_call_id}")
            
            # TODO: Implement proper indexing by modal_call_id in database
            # For now, return None to avoid expensive searches
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Error finding job by modal_call_id {modal_call_id}: {e}")
            return None
    
    async def _store_results_to_gcp(self, job_id: str, result_data: Dict[str, Any]):
        """Store job results to GCP storage"""
        
        try:
            # Use existing GCP storage service
            await gcp_storage_service.store_job_results(
                job_id=job_id,
                result_data=result_data,
                task_type='protein_ligand_binding',  # Default task type
                storage_path=f"jobs/{job_id}"
            )
            
            logger.debug(f"üíæ Stored results for job {job_id} to GCP")
            
        except Exception as e:
            logger.error(f"‚ùå Error storing results to GCP for job {job_id}: {e}")
            # Don't fail the completion for storage errors
    
    def _update_processing_metrics(self, processing_time: float):
        """Update processing time metrics"""
        
        self.processing_times.append(processing_time)
        
        # Keep only recent processing times
        if len(self.processing_times) > self.max_processing_times:
            self.processing_times = self.processing_times[-self.max_processing_times:]
        
        # Update average
        self.webhook_stats.average_processing_time = sum(self.processing_times) / len(self.processing_times)
    
    async def start_fallback_polling(self):
        """Start minimal fallback polling as safety net"""
        
        if not self.polling_enabled:
            logger.info("üîÑ Fallback polling disabled")
            return
        
        logger.info(f"üîÑ Starting fallback polling every {self.polling_interval}s")
        
        while self.polling_enabled:
            try:
                await asyncio.sleep(self.polling_interval)
                await self._poll_for_missed_completions()
                
            except asyncio.CancelledError:
                logger.info("üõë Fallback polling cancelled")
                break
            except Exception as e:
                logger.error(f"‚ùå Error in fallback polling: {e}")
                # Continue polling despite errors
    
    async def _poll_for_missed_completions(self):
        """Minimal polling to catch any missed webhook completions"""
        
        try:
            current_time = time.time()
            
            # Only poll if we haven't polled recently
            if current_time - self.last_poll_time < self.polling_interval:
                return
            
            self.last_poll_time = current_time
            
            logger.debug("üîç Checking for missed completions...")
            
            # Get jobs that are marked as 'running' but might have completed
            # This is a safety net for webhook failures
            
            running_jobs = await self._get_long_running_jobs()
            
            if not running_jobs:
                logger.debug("‚úÖ No long-running jobs found")
                return
            
            logger.info(f"üîç Checking {len(running_jobs)} long-running jobs")
            
            # Check status of long-running jobs
            checked_count = 0
            for job in running_jobs[:10]:  # Limit to 10 jobs per poll
                modal_call_id = job.get('modal_call_id')
                if modal_call_id and modal_call_id not in self.processed_call_ids:
                    await self._check_single_job_status(job)
                    checked_count += 1
            
            logger.debug(f"‚úÖ Checked {checked_count} jobs via fallback polling")
            
        except Exception as e:
            logger.error(f"‚ùå Error in fallback polling: {e}")
    
    async def _get_long_running_jobs(self) -> List[Dict[str, Any]]:
        """Get jobs that have been running for a long time"""
        
        try:
            # Get jobs that have been running for more than 30 minutes
            cutoff_time = time.time() - (30 * 60)  # 30 minutes ago
            
            # This is a simplified query - in production you'd want proper indexing
            # TODO: Implement efficient query for long-running jobs
            
            return []  # Return empty for now to avoid expensive queries
            
        except Exception as e:
            logger.error(f"‚ùå Error getting long-running jobs: {e}")
            return []
    
    async def _check_single_job_status(self, job: Dict[str, Any]):
        """Check status of a single job that might have been missed"""
        
        try:
            job_id = job['id']
            modal_call_id = job.get('modal_call_id')
            
            if not modal_call_id:
                return
            
            # This would check Modal directly for job status
            # For now, we'll skip this to avoid API overhead
            logger.debug(f"üîç Would check status for job {job_id}")
            
        except Exception as e:
            logger.error(f"‚ùå Error checking single job status: {e}")
    
    def get_webhook_stats(self) -> Dict[str, Any]:
        """Get webhook performance statistics"""
        
        return {
            'total_webhooks_received': self.webhook_stats.total_webhooks_received,
            'successful_completions': self.webhook_stats.successful_completions,
            'failed_completions': self.webhook_stats.failed_completions,
            'duplicate_webhooks': self.webhook_stats.duplicate_webhooks,
            'processing_errors': self.webhook_stats.processing_errors,
            'average_processing_time_ms': self.webhook_stats.average_processing_time * 1000,
            'last_webhook_time': self.webhook_stats.last_webhook_time.isoformat() if self.webhook_stats.last_webhook_time else None,
            'processed_call_ids_count': len(self.processed_call_ids),
            'webhook_success_rate': (
                self.webhook_stats.successful_completions / 
                max(self.webhook_stats.total_webhooks_received, 1) * 100
            ),
            'polling_enabled': self.polling_enabled,
            'polling_interval_seconds': self.polling_interval
        }
    
    async def cleanup_old_call_ids(self, max_age_hours: int = 24):
        """Clean up old call IDs to prevent memory growth"""
        
        try:
            # In production, you'd want to implement a proper TTL mechanism
            # For now, we'll just limit the size of the set
            
            if len(self.processed_call_ids) > 10000:
                # Keep only the most recent 5000 call IDs
                call_ids_list = list(self.processed_call_ids)
                self.processed_call_ids = set(call_ids_list[-5000:])
                logger.info(f"üßπ Cleaned up old call IDs, keeping {len(self.processed_call_ids)}")
            
        except Exception as e:
            logger.error(f"‚ùå Error cleaning up call IDs: {e}")
    
    def disable_polling(self):
        """Disable fallback polling (webhook-only mode)"""
        self.polling_enabled = False
        logger.info("üîÑ Fallback polling disabled - webhook-only mode")
    
    def enable_polling(self, interval_seconds: int = 300):
        """Enable fallback polling with custom interval"""
        self.polling_enabled = True
        self.polling_interval = interval_seconds
        logger.info(f"üîÑ Fallback polling enabled - {interval_seconds}s interval")

# Global singleton
webhook_completion_checker = WebhookCompletionChecker()

# Integration functions for batch aware completion checker
BatchAwareCompletionChecker.on_job_failure = lambda self, job_id, error_data: logger.info(f"‚ùå Job {job_id} failed: {error_data.get('message', 'Unknown error')}")

async def start_webhook_completion_service():
    """Start the webhook completion service with fallback polling"""
    
    logger.info("üöÄ Starting webhook completion service...")
    
    # Start fallback polling in background
    polling_task = asyncio.create_task(webhook_completion_checker.start_fallback_polling())
    
    # Cleanup task
    cleanup_task = asyncio.create_task(_periodic_cleanup())
    
    logger.info("‚úÖ Webhook completion service started")
    
    return polling_task, cleanup_task

async def _periodic_cleanup():
    """Periodic cleanup of old data"""
    
    while True:
        try:
            await asyncio.sleep(3600)  # Run every hour
            await webhook_completion_checker.cleanup_old_call_ids()
            
        except asyncio.CancelledError:
            logger.info("üõë Periodic cleanup cancelled")
            break
        except Exception as e:
            logger.error(f"‚ùå Error in periodic cleanup: {e}")
            # Continue despite errors